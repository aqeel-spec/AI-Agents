{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b146d2",
   "metadata": {},
   "source": [
    "# **Running agents**\n",
    "\n",
    "You can run agents via the Runner class. You have 3 options:\n",
    "\n",
    "- `Runner.run()`, which runs async and returns a `RunResult`.\n",
    "- `Runner.run_sync()`, which is a sync method and just runs `.run()` under the hood.\n",
    "- `Runner.run_streamed()`, which runs async and returns a `RunResultStreaming`. It calls the LLM in streaming mode, and streams those events to you as they are received.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f01757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env variables for using custom llm \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not gemini_api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY environment variable is not set. Please set it to your Gemini API key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d155d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load config from openai_agents.config import OpenAIChatCompletionsModel, AsyncOpenAI\n",
    "from agents import OpenAIChatCompletionsModel, AsyncOpenAI\n",
    "\n",
    "async def create_gemini_config():\n",
    "    \"\"\"Create Gemini configuration\"\"\"\n",
    "    if not gemini_api_key:\n",
    "        return None, None\n",
    "    \n",
    "    provider = AsyncOpenAI(\n",
    "        api_key=gemini_api_key,\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "    \n",
    "    model = OpenAIChatCompletionsModel(\n",
    "        model=\"gemini-2.0-flash-exp\",\n",
    "        openai_client=provider\n",
    "    )\n",
    "    \n",
    "    print(f\"Using Gemini model: *** {model.model} ***\")\n",
    "    \n",
    "    return provider, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c155f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Gemini model: *** gemini-2.0-flash-exp ***\n"
     ]
    }
   ],
   "source": [
    "# now create run config\n",
    "from agents import RunConfig\n",
    "\n",
    "provider, model = await create_gemini_config()\n",
    "run_config = RunConfig(\n",
    "    model=model,\n",
    "    model_provider=provider,\n",
    "    tracing_disabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a9e2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b83b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call the same again,\n",
      "A loop within the code,\n",
      "Ending is the key.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n",
    "\n",
    "result = await Runner.run(agent, \"Write a haiku about recursion in programming.\", run_config=run_config)\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3580c64",
   "metadata": {},
   "source": [
    "### Markdown documentation about Runner.run() default behavior\n",
    "\n",
    "#### Runner.run() Default Status and Behavior\n",
    "\n",
    "#### Execution Model\n",
    "- **Asynchronous**: `Runner.run()` is an async function that must be awaited\n",
    "- **Returns**: `RunResult` object containing the agent's response\n",
    "- **Non-blocking**: Allows other async operations to run concurrently\n",
    "\n",
    "### Usage Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc762913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call the same again,\n",
      "Smaller problem takes the lead,\n",
      "Base case sets us free. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example #01\n",
    "# Runner.run(), which runs async and returns a RunResult.\n",
    "async def main(prompt : str):\n",
    "    agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n",
    "\n",
    "    result = await Runner.run(agent, prompt, run_config=run_config)\n",
    "    return result.final_output\n",
    "    # Code within the code,\n",
    "    # Functions calling themselves,\n",
    "    # Infinite loop's dance\n",
    "    \n",
    "result = await main(\"Write a haiku about recursion in programming.\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ce230c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gray skies weep, the world is washed, a gentle, soothing sleep.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await main(\"Write a poem of 10 words in rainy day.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c68782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI model: *** gpt-4o-mini ***\n"
     ]
    }
   ],
   "source": [
    "from g_config import create_openai_config\n",
    "\n",
    "ai_provider, ai_model = create_openai_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "938e0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai configrations\n",
    "opainai_run_config = RunConfig(\n",
    "    model=ai_model,\n",
    "    model_provider=ai_provider,\n",
    "    tracing_disabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.responses import ResponseTextDeltaEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream agent with gpt response\n",
    "stream_agent = Agent(\n",
    "    name=\"StreamAgent\",\n",
    "    instructions=\"You are a streaming assistant that provides responses in real-time.\"\n",
    ")\n",
    "\n",
    "result = Runner.run_streamed(stream_agent, input=\"Please tell me 5 jokes.\", run_config=opainai_run_config)\n",
    "\n",
    "async for event in result.stream_events():\n",
    "    if event.type == \"raw_response_event\":\n",
    "        # Handle different types of response events\n",
    "        try:\n",
    "            if hasattr(event.data, 'delta') and event.data.delta:\n",
    "                # Handle delta events (streaming content)\n",
    "                if hasattr(event.data.delta, 'content'):\n",
    "                    print(event.data.delta.content, end=\"\", flush=True)\n",
    "                elif hasattr(event.data.delta, 'text'):\n",
    "                    print(event.data.delta.text, end=\"\", flush=True)\n",
    "            elif hasattr(event.data, 'content'):\n",
    "                # Handle direct content\n",
    "                print(event.data.content, end=\"\", flush=True)\n",
    "        except Exception as e:\n",
    "            # Skip problematic events and continue streaming\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d095241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
